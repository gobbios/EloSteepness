---
title: "`EloSteepness` - a brief tutorial"
subtitle: "`r paste0('(package version ', as.character(packageVersion('EloSteepness')), ')', sep = '')`"
output: 
  rmarkdown::pdf_document:
    toc: true
    toc_depth: 2
editor_options: 
  chunk_output_type: console
knit: (function(inputFile, encoding, ...) {
    rmarkdown::render(inputFile, encoding = encoding, output_dir = "pdf_files/")
  })
---


```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r, echo=FALSE}
suppressPackageStartupMessages(library(EloSteepness))
```


# prelims

`EloSteepness` is a package that allows estimating steepness of dominance hierarchies from interaction networks.
It does so by estimating Bayesian Elo-ratings, from which the steepness metric can be calculated.
The major difference from classic approaches is that we obtain posterior steepness *distributions*, not point estimates.
More details on the theoretical background can be found in the accompanying preprint/paper.

Also included in this package is a version of steepness that is based on David's scores, but also with a Bayesian flavor.
It turns out that the performance of this approach is somewhat below that of the Elo-based steepness, but still better than the classical algorithms.
Also here the result is a posterior distribution.
This latter approach will be featured only in passing in this tutorial, but in general, all the functions that work with the Elo-based algorithm will also work in a very similar way with the David's score-based steepness.

More interestingly, the package also contains functions that allow extraction of the raw individual scores (either Bayesian Elo-ratings or David's scores), although this is probably only of secondary interest when focusing on *steepness*.

Another batch of functions in the package relate to method evaluation.
These are currently ignored in this document.
They are relevant for replicating the simulations and analyses presented in the paper.

# installing `EloSteepness`

The first requirement is that you use a fairly recent version of `R`, i.e. at least `R 3.5.0`, no way around that.
To find which version you have, do this:

```{r}
R.version$version.string
```

If this returns at least 3.5.0 (or as in my case "\texttt{`r R.version$version.string`}") all is good.
Otherwise you need to update `R`, which might be a good idea anyway.

In order to get the package up an running you need a working installation of `rstan`.
This in turn requires `stan` to be installed but this is taken care of during the setup of the `rstan` package.
The easiest way of doing all this is to install the `brms` package.\footnote{\texttt{brms} is not actually required for \texttt{EloSteepness} to work, but it handles the installation of \texttt{rstan} and friends very conveniently.}
If you already have `brms` (or `rstan`) then you are probably good to go.
If not, then execute the following command and if asked for whether you want to install packages *from source* select 'no' (unless you know what you are doing of course).

```{r, eval = FALSE}
install.packages("brms")
```

The only other thing you need are two more packages, `EloRating` and `aniDom`, which are easy to install:

```{r, eval = FALSE}
install.packages("EloRating")
install.packages("aniDom")
```

With this done, you can install `EloSteepness`.
For the moment, this works only via a local file that can be downloaded here [https://owncloud.dpz.eu/index.php/s/N8VW9a9QIVIuIqZ](https://owncloud.dpz.eu/index.php/s/N8VW9a9QIVIuIqZ).
In the near future I will make the package publicly available via GitHub and hopefully also via CRAN.

Which file to choose from the three in the folder depends on your OS and your level of adventurousness. 
Download the one you need (don't unpack it!), and remember the path you saved it to...

If you are on Windows, go for the `.zip` file and run (and don't forget to change the path):
```{r, eval = FALSE}
install.packages("C:/where/i/saved/my/file/EloSteepness_0.2.0.zip", 
                 repos = NULL, type = "win.binary")
```

If you are on MacOS, go for the `.tgz` file and run (and don't forget to change the path):
```{r, eval = FALSE}
install.packages("~/where/i/saved/my/file/EloSteepness_0.2.0.tgz", 
                 repos = NULL, type = "mac.binary")
```

And finally, if you are on Linux and/or feel adventurous take the `.tar.gz` file:
```{r, eval = FALSE}
install.packages("~/Documents/EloSteepness_0.2.0.tar.gz", 
                 repos = NULL, type = "source")
```

And for good measure at this point it might be a good idea to restart your computer, or at least restart R (or RStudio).








## more on data density

To illustrate some more of the underlying intuitiveness of this system let's run a little experiment/simulation.
The core idea is here is to show how data density informs our assessment of uncertainty.
With more data, uncertainties should become smaller (narrower posterior distributions and credible intervals).
With less data, uncertainties should become larger (wider posteriors and credible intervals).

We will take the same badger data set, but pretend we have observed only for half the time and obtained only half the data (i.e. half the interactions).
We simulate this by dividing the observations in each cell by 2 (and we round .5 randomly up or down).\footnote{we just have to make sure that in the new matrix *d* keeps its one interaction, because otherwise *d* would be dropped (as would any individual without any interactions observed), but we want to have equal group sizes}

```{r}
data("dommats", package = "EloRating")
mat <- dommats$badgers

set.seed(123)
mat1 <- mat / 2
und <- mat1 - floor(mat1) != 0
mat1[und] <- round(mat1[und] + runif(sum(und), -0.1, 0.1))
mat1["f", "d"] <- 1 # just make sure that 'd' keeps its one loss to 'f'
```

And then we also pretend that we observed for twice the amount of time by just doubling all values in `mat`.

```{r}
mat2 <- mat * 2
```

Now we just need redo the calculations for those two 'new' data sets.
Note that I set `cores = 4` as my PC has 4 cores, to speed up things.

```{r elo_steep_02, eval =TRUE, cache=TRUE}
elo_res_half <- elo_steepness_from_matrix(mat = mat1, cores = 4)
elo_res_doubled <- elo_steepness_from_matrix(mat = mat2, cores = 4)
```


Then we visualize the posteriors of steepness and the individual scores. 

```{r elo02_plot1, fig.retina= 2, echo=2:10, fig.width=9, fig.height=6.5, out.width="90%", fig.align='center'}
par(mfrow = c(2, 3), family = "serif")
plot_steepness(elo_res_half)
plot_steepness(elo_res)
plot_steepness(elo_res_doubled)

plot_scores(elo_res_half, color = my_colors)
plot_scores(elo_res, color = my_colors)
plot_scores(elo_res_doubled, color = my_colors)
```


In this plot, the original data set is in the center column, with the reduced data on the left and the doubled data on the right.
The first thing I'd notice is that for the reduced steepness we now have 50 randomized sequences, which is due to the function's (intentional) default behavior of running more randomizations with smaller data sets.
The second thing to note here is that the credible intervals become narrower from left to right, i.e. from less to more data.
Admittedly though, this is not extremely obvious in this example.
But you can verify it by looking at the output of `steepness_precis()` for each of the steepness objects (`elo_res_half`, `elo_res` and `elo_res_doubled`).

As far as the individual scores are concerned, the same general pattern of credible intervals becoming narrower with more data emerges.
Again, this is not overly obvious, other than for the individual with the highest score (blue on the right).
What is interesting here is that the overlap between adjacent individuals does not seem to become smaller, i.e. there are 'clusters' of individuals, where even more data doesn't help to separate their scores.


```{r, echo=FALSE}
x <- cbind(# elo_res_doubled$ids, 
      scores(elo_res_half)[, 7] - scores(elo_res_half)[, 6],
      scores(elo_res)[, 7] - scores(elo_res)[, 6],
      scores(elo_res_doubled)[, 7] - scores(elo_res_doubled)[, 6])
x[, ] <- sprintf("%.2f", x)
```



```{r, echo=FALSE, eval = FALSE}
diff(as.numeric(steepness_precis(elo_res_half)[1, c("q045", "q955")]))
diff(as.numeric(steepness_precis(elo_res)[1, c("q045", "q955")]))
diff(as.numeric(steepness_precis(elo_res_doubled)[1, c("q045", "q955")]))

diff(as.numeric(steepness_precis(elo_res_half)[1, c("q250", "q750")]))
diff(as.numeric(steepness_precis(elo_res)[1, c("q250", "q750")]))
diff(as.numeric(steepness_precis(elo_res_doubled)[1, c("q250", "q750")]))
```



## a final example

Finally, we go for another example.
This one actually uses simulated data.
The major point of this example is to illustrate the idea of more data generally equaling less uncertainty (i.e. narrower distributions).
The second point of this example is that posteriors do not necessarily look symmetric and can have indeed fairly odd shapes.

We also repeat our experiment about 'increasing the amount of data'.
But this time we just multiply, so as not to have to bother with rounding up or down.

```{r simu_elo_ex_1, eval = TRUE, cache = TRUE}
set.seed(123)
# generate matrices
m1 <- simple_steep_gen(n_ind = 6, n_int = 40, steep = 0.9)$matrix
m2 <- m1 * 2
m5 <- m1 * 5
# calculate steepness
r1 <- elo_steepness_from_matrix(mat = m1, n_rand = 10, cores = 4)
r2 <- elo_steepness_from_matrix(mat = m2, n_rand = 10, cores = 4)
r5 <- elo_steepness_from_matrix(mat = m5, n_rand = 10, cores = 4)
```



```{r simu_elo_ex_plot1, fig.retina= 2, eval = TRUE, echo=2:15, fig.width=9, fig.height=5.8, out.width="90%", fig.align='center'}
par(mfrow = c(2, 3), family = "serif", mar = c(3.5, 2.5, 1, 1))
mycols <- hcl.colors(6, palette = "Dark 2", alpha = 0.7)

plot_steepness(r1)
plot_steepness(r2)
plot_steepness(r5)

plot_scores(r1, color = mycols)
plot_scores(r2, color = mycols)
plot_scores(r5, color = mycols)
```










\newpage

# Bayesian David's scores

Using David's scores instead of Elo-rating-based cumulative winning probabilities works pretty much in the same way.
The one key difference is that there is no data sequence to be randomized.
Therefore, we don't need to set randomizations because the matrix is just what it is\footnote{as opposed to Elo-rating, which requires translating the matrix into sequences of interactions for which the actual order of interactions is not known and hence randomized multiple times}.

The workhorse function is `davids_steepness()`, which only requires the `mat=` argument to be specified.
We use again the badgers data set to illustrate.

Given the results of the method comparison in the paper, I would recommend not to interpret the steepness value (based on Bayesian David's scores) of this exercise as meaningful given that there are still a substantial amount of unknown relationships in this data set.



```{r}
data("dommats", package = "EloRating")
mat <- dommats$badgers
```


```{r david_steep_01, cache=TRUE, eval = TRUE}
david_res <- davids_steepness(mat, refresh = 0)
```

Since there are no randomized sequences involved, the steepness posterior does only show the results of the single set of samples along with some descriptives of the posterior.

```{r david_steep_01_plot1, fig.retina= 2, fig.height=4.5, fig.width=7, out.width="70%", fig.align='center', echo=2:2}
par(family = "serif", mar = c(3.5, 2.5, 1, 1))
plot_steepness(david_res)
```


```{r}
summary(david_res)
```


The other functions that are available for Elo-rating based steepness are also available for the David's score-based steepness:

```{r}
round(steepness_precis(david_res), 2)
```


```{r david_steep_01_plot2, fig.retina= 2, fig.height=4.5, fig.width=7, out.width="70%", fig.align='center', echo=2:2}
par(family = "serif", mar = c(3.5, 2.5, 1, 1))
plot_scores(david_res)
```



```{r, eval=FALSE}
scores(david_res)
```

```{r, results='asis', echo=FALSE}
my_scores <- scores(david_res)
knitr::kable(my_scores, digits = 3)
```

```{r david_steep_01_plot3, fig.retina= 2, fig.height=4.5, fig.width=7, out.width="70%", fig.align='center', echo=2:2}
par(family = "serif", mgp = c(2, 0.8, 0), mar = c(3.5, 3.5, 1, 1), tcl = -0.4)
plot_steepness_regression(david_res, width_fac = 1)
```






